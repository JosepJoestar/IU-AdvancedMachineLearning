{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXpnXB2U73aB"
   },
   "source": [
    "# Recurrent Networks - AML | Innopolis University\n",
    "# Josep de Cid Rodríguez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q0HnihHW77jc"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ClJXj10C8Nbf"
   },
   "outputs": [],
   "source": [
    "! cp /gdrive/My\\ Drive/IU/AML/Labs/Lab5/train_eng.csv .\n",
    "! cp /gdrive/My\\ Drive/IU/AML/Labs/Lab5/test_eng.csv .\n",
    "\n",
    "! cp /gdrive/My\\ Drive/IU/AML/Labs/Lab5/train_rus.csv .\n",
    "! cp /gdrive/My\\ Drive/IU/AML/Labs/Lab5/test_rus.csv ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9CAuECi_73aF"
   },
   "source": [
    "## Data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VHZTVO8t73aC"
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "import time\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBJ_KBwl8kiz"
   },
   "source": [
    "Read of both *CSV* files, splitting our data in features (**names** column) and classification targets (**gender** column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ir-C_vwT73aG"
   },
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    df = pd.read_csv(path)\n",
    "    x = df.values[:, 0]\n",
    "    y = df.values[:, 1]\n",
    "    return df, x, y\n",
    "\n",
    "train_df =  pd.read_csv('train_eng.csv')\n",
    "test_df = pd.read_csv('test_eng.csv')\n",
    "all_df = pd.concat((train_df, test_df), axis=0)\n",
    "\n",
    "train_ru_df = pd.read_csv('train_rus.csv')\n",
    "test_ru_df = pd.read_csv('test_rus.csv')\n",
    "\n",
    "train = train_df.values\n",
    "test = test_df.values\n",
    "\n",
    "train_rus = train_ru_df.values\n",
    "test_rus = test_ru_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6D_ZGAl73aK"
   },
   "source": [
    "We will apply the following preprocessing to our data:\n",
    "1. Map every dictionary letter to an unique positive ID s.t. e.g. `Elizabeth` → `[5 38 35 52 27 28 31 46 34]`\n",
    "2. Apply padding `0` to the samples to have the same length in all → `[5 38 35 52 27 28 31 46 34 0 ... 0]`\n",
    "3. Encode our labels as `0` if *Female* or `1` if *Male*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w8F3j9b373aK"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(values, train=False, male_label='M', max_seq=None):\n",
    "    if train:\n",
    "        # Sort training names by length increasing\n",
    "        values = np.stack(sorted(list(values), key=lambda x: len(x[0])))\n",
    "    \n",
    "    x, y = values[:, 0], values[:, 1:]\n",
    "\n",
    "    # Create vocabulary mapping letter -> ID\n",
    "    unique = sorted(set(\"\".join(values[:, 0])))\n",
    "    vocab = dict(zip(unique, range(1, len(unique) + 1)))\n",
    "\n",
    "    # Max length of our sequences in the model\n",
    "    if max_seq is None:\n",
    "        max_seq = max(map(len, x))\n",
    "\n",
    "    # Maps name to array of ids for every letter\n",
    "    encode_letter = lambda letter: vocab[letter]\n",
    "    encode_name = lambda name: list(map(encode_letter, name))\n",
    "    encode_names = lambda names: list(map(encode_name, names))\n",
    "\n",
    "    x_ids = encode_names(x)\n",
    "\n",
    "    # Adds padding to names in id form to have length max_seq\n",
    "    for idx, name in enumerate(x_ids):\n",
    "        padding = max_seq - len(name)\n",
    "        x_ids[idx] = np.array(name + [0]*padding)\n",
    "\n",
    "    # Encode targets in 0 (Female) and 1 (Male)\n",
    "    binaryze_targets = lambda targets: [[int(x[0] == male_label)] for x in targets]\n",
    "    y_bin = np.array(binaryze_targets(y))\n",
    "\n",
    "    return x_ids, y_bin, max_seq, vocab\n",
    "    \n",
    "x_train_ids, y_train_bin, max_seq, vocab = preprocess_data(train, train=True)\n",
    "x_test_ids, y_test_bin, _, _ = preprocess_data(test, max_seq=max_seq)\n",
    "\n",
    "x_train_rus_ids, y_train_rus_bin, max_seq_rus, vocab_rus = preprocess_data(train_rus, train=True, male_label='М')\n",
    "x_test_rus_ids, y_test_rus_bin, _, _ = preprocess_data(test_rus, male_label='М', max_seq=max_seq_rus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yvhyTNJZ73aN"
   },
   "source": [
    "## Models\n",
    "\n",
    "We will implement two parametrizable models in order to tune the hyperparameters and choose the most appropiate one. We will start with some standard Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4zPfS9VQ73aP"
   },
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, x_train, y_train, x_val, y_val, print_mode=True):\n",
    "        # Reset previous graphs\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Model Data \n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "        \n",
    "        # Print mode (text | plot)\n",
    "        self.print_mode = print_mode\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def create_graph(self):\n",
    "        return\n",
    "    \n",
    "    def print_trainable_parameters(self):\n",
    "        params = np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n",
    "        print('Trainable parameters {}'.format(params))\n",
    "            \n",
    "    def train(self, epochs=100, batch_size=256, patience=25):\n",
    "        training_start_time = time.time()\n",
    "        \n",
    "        train_accuracies = []\n",
    "        test_accuracies = []\n",
    "        \n",
    "        worse_epoch_count = 0\n",
    "        best_train_accuracy = 0\n",
    "        best_test_accuracy = 0\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "            \n",
    "            for epoch in range(1, epochs + 1):\n",
    "                if self.print_mode:\n",
    "                    progress = tf.keras.utils.Progbar(target=len(self.x_train),\n",
    "                                                      stateful_metrics=['batch loss', 'time'],\n",
    "                                                      width=30, interval=0.5)\n",
    "                \n",
    "                start_time = time.time()\n",
    "                if self.print_mode:\n",
    "                    print('> Epoch {}:'.format(epoch))\n",
    "                \n",
    "                for batch_idx, batch in enumerate(self._next_batch(self.x_train, self.y_train, batch_size, shuffle=True)):\n",
    "                    features, targets = batch\n",
    "                    d = {self.names: features, self.genders: targets}\n",
    "                    loss, _ = sess.run([self.loss, self.optimize], feed_dict=d)\n",
    "                    \n",
    "                    if self.print_mode:\n",
    "                        elapsed_time = time.time() - start_time\n",
    "                        progress.update(batch_idx * batch_size,\n",
    "                                        values=[('time', elapsed_time), ('batch loss', loss), ('epoch loss', loss)])\n",
    "                    \n",
    "                if self.print_mode:\n",
    "                    progress.update(len(self.x_train), values=[('batch loss', loss), ('epoch loss', loss)])\n",
    "\n",
    "                train_accuracy = sess.run(self.accuracy, feed_dict={self.names: self.x_train, self.genders: self.y_train})\n",
    "                test_accuracy = sess.run(self.accuracy, feed_dict={self.names: self.x_val, self.genders: self.y_val})\n",
    "                \n",
    "                if self.print_mode:\n",
    "                    print('Epoch {:2} | Training set accuracy = {:.4f}, Test set accuracy = {:.4f}'\n",
    "                          .format(epoch, train_accuracy, test_accuracy))\n",
    "                \n",
    "                train_accuracies.append(train_accuracy)\n",
    "                test_accuracies.append(test_accuracy)\n",
    "                \n",
    "                best_train_accuracy = max(train_accuracy, best_train_accuracy)\n",
    "                if test_accuracy > best_test_accuracy:\n",
    "                    worse_epoch_count = 0\n",
    "                    best_test_accuracy = test_accuracy\n",
    "                else:\n",
    "                    worse_epoch_count += 1\n",
    "                    if worse_epoch_count == patience:\n",
    "                        if self.print_mode:\n",
    "                            print('Early stopping at epoch {}'.format(epoch))\n",
    "                        break\n",
    "\n",
    "        return best_train_accuracy, train_accuracies, best_test_accuracy, test_accuracies, time.time() - training_start_time\n",
    "                \n",
    "    def _next_batch(self, x, y, batch_size, shuffle=False):\n",
    "        position = 0\n",
    "        while position + batch_size < len(x):\n",
    "            offset = position + batch_size\n",
    "            yield x[position:offset], y[position:offset]\n",
    "            position = offset\n",
    "        yield x[position:], y[position:]\n",
    "        \n",
    "    def _create_graph_input(self, input_dim, vocab_dim, emb_dim):\n",
    "        # Placeholders for input and targets\n",
    "        self.names = tf.placeholder(tf.int32, shape=[None, input_dim], name='Names')\n",
    "        self.genders = tf.placeholder(tf.float32, shape=[None, 1], name='Genders')\n",
    "\n",
    "        # Embedding Matrix (0-pad is not a variable, remains 0)\n",
    "        padding_vector = tf.zeros(shape=(1, emb_dim), dtype=tf.float32, name='ZeroPadding')\n",
    "        symbol_embedding = tf.get_variable('W', shape=(vocab_dim, emb_dim), dtype=tf.float32)\n",
    "        symbol_embedding = tf.concat([padding_vector, symbol_embedding], axis=0)\n",
    "    \n",
    "        # Word embeddings\n",
    "        return tf.nn.embedding_lookup(symbol_embedding, self.names)\n",
    "    \n",
    "    def _create_graph_output(self, last_layer, lr):\n",
    "        # Dense layer with binary output\n",
    "        logits = tf.layers.dense(last_layer, 1)\n",
    "        \n",
    "        # Loss & Optimization\n",
    "        logits_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=self.genders)\n",
    "        self.loss = tf.reduce_mean(logits_loss)\n",
    "        optimizer = tf.contrib.opt.LazyAdamOptimizer(lr)\n",
    "        self.optimize = optimizer.minimize(self.loss)\n",
    "\n",
    "        # Prediction & Accuracy\n",
    "        self.predictions = tf.round(tf.sigmoid(logits))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.predictions, self.genders), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KUh0S34y73aT"
   },
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sjl8Ej_n73aU"
   },
   "outputs": [],
   "source": [
    "class RNN(NN):\n",
    "    def create_graph(self, input_dim, vocab_dim, emb_dim=5, lr=0.001,\n",
    "                     unit_layers=[5], activations=['relu'], cell_type=[('LSTM', False)]):\n",
    "        assert len(unit_layers) > 0\n",
    "        assert len(unit_layers) == len(activations)\n",
    "    \n",
    "        # Word embeddings\n",
    "        embedded_names = self._create_graph_input(input_dim, vocab_dim, emb_dim)\n",
    "        \n",
    "        # Add Layers\n",
    "        cell_layers = []\n",
    "        for idx, hidden_dim in enumerate(unit_layers):\n",
    "            if cell_type[idx][0] == 'LSTM':\n",
    "                cell_layers.append(tf.nn.rnn_cell.LSTMCell(hidden_dim, activation=activations[idx],\n",
    "                                                           use_peepholes=cell_type[idx][1]))\n",
    "            else:\n",
    "                cell_layers.append(tf.nn.rnn_cell.GRUCell(hidden_dim, activation=activations[idx]))\n",
    "            \n",
    "        # Multilayer cell\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cell_layers, state_is_tuple=True)\n",
    "\n",
    "        # Dynamic RNN (Dynamic graph with a loop) using defined cell\n",
    "        outputs, _ = tf.nn.dynamic_rnn(cell=cell, inputs=embedded_names, dtype=tf.float32)\n",
    "\n",
    "        # Filter only last timestep output\n",
    "        last_layer = outputs[:, -1, :]\n",
    "        self._create_graph_output(last_layer, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cKrf0PfV2_LJ"
   },
   "outputs": [],
   "source": [
    "model = RNN(x_train_ids, y_train_bin, x_test_ids, y_test_bin)\n",
    "\n",
    "model.create_graph(input_dim=max_seq, vocab_dim=len(vocab), emb_dim=10,\n",
    "                   unit_layers=[20, 10], activations=['relu', 'relu'],\n",
    "                   cell_type=[('LSTM', True), ('LSTM', True)])\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "_ = model.train(epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0lv8d4g_73aW"
   },
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yVVrq8pI73aW"
   },
   "outputs": [],
   "source": [
    "class FNN(NN):\n",
    "    def create_graph(self, input_dim, vocab_dim, emb_dim=5, lr=0.001, unit_layers=[5], activations=['relu'], mode=('flatten', 1)):\n",
    "        # Word embeddings\n",
    "        embedded_names = self._create_graph_input(input_dim, vocab_dim, emb_dim)\n",
    "        flatten_names = self._flatten_input(embedded_names, input_dim, emb_dim, mode)\n",
    "\n",
    "        # Add Dense Layers\n",
    "        layers = [flatten_names]\n",
    "        for idx, units in enumerate(unit_layers):\n",
    "            layers.append(tf.layers.dense(layers[-1], units, activation=activations[idx]))\n",
    "\n",
    "        last_layer = layers[-1]\n",
    "        self._create_graph_output(last_layer, lr)\n",
    "        \n",
    "    def _flatten_input(self, tensor, input_dim, emb_dim, mode):\n",
    "        assert len(mode) == 2\n",
    "        assert mode[0] in ['flatten', 'max_pool', 'average', 'w_average']\n",
    "        \n",
    "        if mode[0] == 'max_pool':\n",
    "            return tf.reduce_max(tensor, axis=mode[1])\n",
    "        elif mode[0] == 'average':\n",
    "            return tf.reduce_mean(tensor, axis=mode[1])\n",
    "        elif mode == 'w_average':\n",
    "            filt = tf.get_variable('filter', shape=(1, input_dim, 1)\n",
    "                                   if mode[1] == 1 else (1, 1, emb_dim))\n",
    "            return tf.reduce_mean(tensor * filt, axis=mode[1])\n",
    "        else:\n",
    "            return tf.reshape(tensor, shape=(-1, input_dim * emb_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ofazSb3673ab"
   },
   "outputs": [],
   "source": [
    "model = FNN(x_train_ids, y_train_bin, x_test_ids, y_test_bin)\n",
    "\n",
    "model.create_graph(input_dim=max_seq, vocab_dim=len(vocab),\n",
    "                   unit_layers=[102], activations=['sigmoid'])\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "_ = model.train(epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "22n2GsmU73ah"
   },
   "source": [
    "## Hyperparameter Search\n",
    "\n",
    "In this homework, instead of the classic Grid Search, for hyperparameter optimization, we will use Random Search. [Why?](https://analyticsindiamag.com/wp-content/uploads/2018/06/both.png) Because random search has a probability of 95% of finding a combination of parameters within the 5% optima with only 60 iterations ([much faster](https://www.cnblogs.com/yymn/p/4536740.html)). Also compared to other methods it doesn't bog down in local optima. To understand it in a visual way, take a look at the following example:\n",
    "\n",
    "![Grid Search vs Random Search](https://i.stack.imgur.com/cIDuR.png)\n",
    "\n",
    "So let's start defining a function to generate random parameter configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vU1ygdNd73ai"
   },
   "outputs": [],
   "source": [
    "def generate_random_hyperparameters(rnn=True):\n",
    "    '''Generate random learning rate and keep probability'''\n",
    "    # Random normal distribution around 0.001 \n",
    "    learning_rate = 10 ** -np.random.normal(3, 0.5)\n",
    "    \n",
    "    # From 1 to 3-4 layers\n",
    "    max_lay = 3 if rnn else 4\n",
    "    layers = np.random.randint(1, max_lay + 1)\n",
    "    \n",
    "    # 5..20 units/layer in RNN, 50..200 in MLP\n",
    "    l = 5 if rnn else 50\n",
    "    h = 20 if rnn else 200\n",
    "    layers_neurons = np.random.randint(l, h, layers).tolist()\n",
    "    \n",
    "    # Batch size of 128, 256, 512 or 1024\n",
    "    batch_size = np.random.choice([2 ** p for p in range(7, 11)])\n",
    "    \n",
    "    # Activation Tanh, Sigmoid or ReLU\n",
    "    activation = np.random.choice(['tanh', 'sigmoid', 'relu'], layers)\n",
    "    \n",
    "    # RNN ONLY -> LSTM or GRU\n",
    "    cell_types = np.random.choice(['LSTM', 'GRU'], layers)\n",
    "    cell_peepholes = np.random.choice([True, False], layers)\n",
    "    \n",
    "    # MLP ONLY -> Flatten technique\n",
    "    flatten_mode = np.random.choice(['flatten', 'max_pool', 'average', 'w_average'])\n",
    "    flatten_axis = np.random.randint(1, 3)\n",
    "    \n",
    "    return {\n",
    "        'LR': learning_rate,\n",
    "        'LN': layers_neurons,\n",
    "        'AC': activation,\n",
    "        'BS': batch_size,\n",
    "        'CT': list(zip(cell_types, cell_peepholes)),\n",
    "        'FM': (flatten_mode, flatten_axis)\n",
    "    }\n",
    "\n",
    "def configuration_to_label(c, rnn=True):\n",
    "    a_tags = list(map(lambda a: 'T' if a == 'tanh' else ('S' if a == 'sigmoid' else 'R'), c['AC']))\n",
    "    if rnn:\n",
    "        rnn_cell_label = lambda c: 'G' if c[0] == 'GRU' else ('LP' if c[1] else 'L')\n",
    "        layers = ', '.join(map(lambda x: '{}{}{}'.format(rnn_cell_label(x[2]), x[0], x[1]), zip(c['LN'], a_tags, c['CT'])))\n",
    "    else:\n",
    "        layers = ', '.join(map(lambda x: '{}{}'.format(x[0], x[1]), zip(c['LN'], a_tags)))\n",
    "    label = ' LR={:.5f}; BatchSize={}; Layers={}'.format(c['LR'], c['BS'], layers)\n",
    "        \n",
    "    if rnn:\n",
    "        return '(RNN) {}'.format(label)\n",
    "    else:\n",
    "        if c['FM'][0] == 'flatten':\n",
    "            return '(MLP - {}) {}'.format(c['FM'][0], label)\n",
    "        else:\n",
    "            return '(MLP - {}({})) {}'.format(c['FM'][0], c['FM'][1], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eFrLjacX4KuU"
   },
   "outputs": [],
   "source": [
    "def plot_hyperparam_tunning(accuracies):\n",
    "    best_accuracies_configs = sorted(accuracies, key=lambda m: m['best_accuracy']['test'], reverse=True)\n",
    "    best_accuracies_configs_head = best_accuracies_configs[:5]\n",
    "\n",
    "    colors = ['#198181', '#F70022', '#F7904C', '#00ADC9', '#6E50C8']\n",
    "    max_epochs = max(map(lambda c: len(c['accuracies']['train']), best_accuracies_configs_head))\n",
    "    max_train_accuracy = max(map(lambda c: c['best_accuracy']['train'], best_accuracies_configs_head))\n",
    "    max_test_accuracy = max(map(lambda c: c['best_accuracy']['test'], best_accuracies_configs_head))\n",
    "\n",
    "    _, (ax_train, ax_test) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    plt.subplots_adjust(hspace=0.25)\n",
    "\n",
    "    for i, m in enumerate(best_accuracies_configs_head):\n",
    "        x = list(range(1, len(m['accuracies']['train']) + 1))\n",
    "    \n",
    "        ax_train.plot(x, m['accuracies']['train'], label=m['label'], color=colors[i])\n",
    "        ax_train.axvline(len(x), color=colors[i], linestyle='--', linewidth=0.5)\n",
    "    \n",
    "        ax_test.plot(x, m['accuracies']['test'], label=m['label'], color=colors[i])\n",
    "        ax_test.axvline(len(x), color=colors[i], linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax_train.set_title('Train data accuracy')\n",
    "    ax_train.set_ylim(m['accuracies']['train'][0] - 0.01, max_train_accuracy + 0.01)\n",
    "\n",
    "    ax_test.set_title('Test data accuracy')\n",
    "    ax_test.set_ylim(m['accuracies']['test'][0] - 0.01, max_test_accuracy + 0.01)\n",
    "\n",
    "    for axe in (ax_train, ax_test):\n",
    "        axe.set_xlabel('Epoch')\n",
    "        axe.set_ylabel('Accuracy')\n",
    "        axe.set_xlim(0, max_epochs + 2)\n",
    "        axe.legend(loc=4, frameon=True, shadow=True, edgecolor='black')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    accuracies_data = np.array([\n",
    "        [m['best_accuracy']['train'] for m in best_accuracies_configs_head],\\\n",
    "        [m['best_accuracy']['test'] for m in best_accuracies_configs_head],\\\n",
    "        [len(m['accuracies']['train']) for m in best_accuracies_configs_head]\n",
    "    ]).T\n",
    "\n",
    "    accuracies_df = pd.DataFrame(accuracies_data, index=range(1, 6), columns=['Train data', 'Test data', 'Epochs'])\n",
    "    accuracies_df.Epochs = accuracies_df.Epochs.astype('int32')\n",
    "    accuracies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CN_jJqtH2_Le"
   },
   "outputs": [],
   "source": [
    "def random_search(n=60, k=5, rnn=True):\n",
    "    best_accuracies_configs = []\n",
    "    \n",
    "    xk_ids, yk_bin, max_seq, vocab = preprocess_data(all_df.values, train=True)\n",
    "    \n",
    "    # Random iterations\n",
    "    for idx in range(n):\n",
    "        c = generate_random_hyperparameters(rnn=rnn)\n",
    "        c_label = configuration_to_label(c, rnn=rnn)\n",
    "        print('{}.- C = {}'.format(idx + 1, c_label))\n",
    "        \n",
    "        k_train_accs = np.ma.empty((100, k))\n",
    "        k_train_accs.mask = True\n",
    "        \n",
    "        k_test_accs = np.ma.empty((100, k))\n",
    "        k_test_accs.mask = True\n",
    "        \n",
    "        k_train_acc, k_test_acc, k_train_time = [], [], []\n",
    "        \n",
    "        # K-Fold cross validation\n",
    "        for k_idx in range(k):\n",
    "            x_train, x_test, y_train, y_test = train_test_split(xk_ids, yk_bin, test_size=0.2)\n",
    "            \n",
    "            if rnn:\n",
    "                model = RNN(x_train, y_train, x_test, y_test, print_mode=False)\n",
    "                model.create_graph(input_dim=max_seq, vocab_dim=len(vocab), lr=c['LR'], unit_layers=c['LN'], activations=c['AC'], cell_type=c['CT'])\n",
    "            else:\n",
    "                model = FNN(x_train, y_train, x_test, y_test, print_mode=False)\n",
    "                model.create_graph(input_dim=max_seq, vocab_dim=len(vocab), lr=c['LR'], unit_layers=c['LN'], activations=c['AC'], mode=c['FM'])\n",
    "\n",
    "            if k_idx == 0:\n",
    "                model.print_trainable_parameters()\n",
    "            \n",
    "            best_train_acc, train_accs, best_test_acc, test_accs, training_time = model.train(batch_size=c['BS'])\n",
    "            \n",
    "            k_train_accs[:len(train_accs), k_idx] = train_accs\n",
    "            k_train_acc.append(best_train_acc)\n",
    "            \n",
    "            k_test_accs[:len(train_accs), k_idx] = test_accs\n",
    "            k_test_acc.append(best_test_acc)\n",
    "            \n",
    "            k_train_time.append(training_time)\n",
    "            \n",
    "            msg = '\\t{}) Training time: {:.4f}, Train accuracy: {:.6f}, Test accuracy: {:.6f}'\n",
    "            print(msg.format(k_idx + 1, training_time, best_train_acc, best_test_acc))\n",
    "\n",
    "        training_time = np.mean(k_train_time)\n",
    "        best_train_acc = np.mean(k_train_acc)\n",
    "        best_test_acc = np.mean(k_test_acc)\n",
    "            \n",
    "        best_accuracies_configs.append({\n",
    "            'accuracies': {'train': k_train_accs.mean(axis=1), 'test': k_test_accs.mean(axis=1)},\n",
    "            'best_accuracy': {'train': best_train_acc, 'test': best_test_acc},\n",
    "            'time': training_time,\n",
    "            'label': c_label\n",
    "        })\n",
    "        \n",
    "        msg = 'Training time: {:.4f}, Train accuracy: {:.6f}, Test accuracy: {:.6f}'\n",
    "        print(msg.format(training_time, best_train_acc, best_test_acc))\n",
    "\n",
    "        print(50*'-')\n",
    "    \n",
    "    return best_accuracies_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AM2Zb5UG2_Lg"
   },
   "source": [
    "### RNN - Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "107YV3Ln73ak"
   },
   "outputs": [],
   "source": [
    "best_accuracies_rnn_configs = random_search(rnn=True)\n",
    "plot_hyperparam_tunning(best_accuracies_rnn_configs)\n",
    "\n",
    "with open('hyper_rnn.pkl', mode='wb') as f:\n",
    "    pickle.dump(best_accuracies_rnn_configs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hg625sMI2_Ln"
   },
   "source": [
    "### MLP - Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pYHyHAIB2_Lo"
   },
   "outputs": [],
   "source": [
    "best_accuracies_mlp_configs = random_search(rnn=False)\n",
    "plot_hyperparam_tunning(best_accuracies_mlp_configs)\n",
    "\n",
    "with open('hyper_mlp.pkl', mode='wb') as f:\n",
    "    pickle.dump(best_accuracies_mlp_configs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WQKhZIIniDnb"
   },
   "source": [
    "# Russian names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuYwK2msiPPv"
   },
   "outputs": [],
   "source": [
    "model = RNN(x_train_rus_ids, y_train_rus_bin, x_test_rus_ids, y_test_rus_bin)\n",
    "\n",
    "model.create_graph(input_dim=max_seq_rus, vocab_dim=len(vocab_rus), emb_dim=10,\n",
    "                   unit_layers=[20, 10], activations=['elu', 'elu'],\n",
    "                   cell_type=[('GRU', None), ('LSTM', True)])\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "_ = model.train(epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SjMysvIUjdU5"
   },
   "outputs": [],
   "source": [
    "model = FNN(x_train_rus_ids, y_train_rus_bin, x_test_rus_ids, y_test_rus_bin)\n",
    "\n",
    "model.create_graph(input_dim=max_seq_rus, vocab_dim=len(vocab_rus),\n",
    "                   unit_layers=[102], activations=['sigmoid'])\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "_ = model.train(epochs=500)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNN.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
