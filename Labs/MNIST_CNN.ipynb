{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b8mQ9MyDen9L"
   },
   "source": [
    "# MNIST Odd-Even classification with TensorFlow (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7EpNY5G0en9M"
   },
   "source": [
    "## Data loading\n",
    "\n",
    "Let's load the dataset using the Keras API implementation included in TensorFlow.\n",
    "\n",
    "As we only want to classify the images between **odd** and **even** numbers, we will map labels to *1* if odd or *0* if even."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "7dEsM3NSen9N"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Keep original test numbers for displaying misslcasificationsx\n",
    "y_numbers = y_test\n",
    "\n",
    "# Set Even or Odd labels for each sample\n",
    "y_train = np.array(list(map(lambda x: x%2, y_train)), dtype=np.float32)\n",
    "y_test = np.array(list(map(lambda x: x%2, y_test)), dtype=np.float32)\n",
    "\n",
    "# Normalize images dividing by max pixel value (255)\n",
    "x_train = (x_train / 255.0).astype(np.float32)\n",
    "x_test = (x_test / 255.0).astype(np.float32)\n",
    "\n",
    "# Reshape to TF API (#img, rows, cols, channels)\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oVAWymGh61LS"
   },
   "source": [
    "## CNN Model\n",
    "\n",
    "### Neural Network architecture\n",
    "\n",
    "We will use the classical architecture for a CNN (Convolution + Pooling > Convolution + Pooling > Flatten > Dense > Dense)\n",
    "\n",
    "Instead of the common Convolution > ReLU > Pooling, we can apply the ReLU after Pooling step beacuse ReLU(MaxPooling(x)) == MaxPooling(ReLU(x)) and it will be more efficient, as the tensor size will be 75% smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier:\n",
    "    def __init__(self, train_data=None):\n",
    "        data, labels = train_data\n",
    "\n",
    "        # labels = self._transform_labels(labels)\n",
    "        # data = self._flatten_input(data)\n",
    "        \n",
    "        self.train_data = (data, labels)\n",
    "\n",
    "        self.assemble_graph()\n",
    "\n",
    "        self._open_session()\n",
    "        \n",
    "        self._draw_graph_tensorboard()\n",
    "\n",
    "        if train_data:\n",
    "            self.train()     \n",
    "\n",
    "   \n",
    "    def assemble_graph(self, learning_rate = 0.02):\n",
    "        #### Placeholders/Variables/Constants\n",
    "        \n",
    "        self.X = tf.placeholder(name='X', dtype=tf.float32, shape=(None, 28, 28, 1))\n",
    "        self.Y = tf.placeholder(name='Y', dtype=tf.float32, shape=(None,))\n",
    "        self.L = tf.reshape(self.Y, shape=(-1, 1))\n",
    "        \n",
    "        #### Layers\n",
    "        \n",
    "        # 1st Convolutional Layer 3x5x5 Kernel + MaxPooling 2x2 + ReLu\n",
    "        conv1 = tf.layers.conv2d(self.X, filters=3, kernel_size=(5, 5), padding='same')\n",
    "        pool1 = tf.layers.max_pooling2d(conv1, pool_size=(2, 2), strides=2)\n",
    "        relu1 = tf.nn.relu(pool1)\n",
    "        \n",
    "        # 2nd Convolutional Layer 3x5x5 Kernel + MaxPooling 2x2 + ReLu\n",
    "        conv2 = tf.layers.conv2d(relu1, filters=3, kernel_size=(5, 5), padding='same')\n",
    "        pool2 = tf.layers.max_pooling2d(conv2, pool_size=(2, 2), strides=2)\n",
    "        relu2 = tf.nn.relu(pool2)\n",
    "        \n",
    "        # Flatten data into (-1, 7*7*3 tensor)\n",
    "        flatten = tf.reshape(relu2, shape=(-1, 7*7*3))\n",
    "        \n",
    "        # Dense Layer 1 with 4 neurons\n",
    "        dense1 = tf.layers.dense(flatten, units=4, activation=tf.nn.relu)\n",
    "        \n",
    "        # Dense Layer 2 with 1 neurons (our final output)\n",
    "        output_layer = tf.layers.dense(dense1, units=1, activation=None)\n",
    "        \n",
    "        #### Optimizer, Loss, Predictions and Accuracy\n",
    "        \n",
    "        # Cross-Entropy as loss function\n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.L, logits=output_layer)\n",
    "        self.cost = tf.reduce_mean(cross_entropy)\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(self.cost)\n",
    "        \n",
    "        # Apply sigmoid and round to get the predicted class\n",
    "        self.predicted = tf.nn.sigmoid(output_layer)\n",
    "        correct_pred = tf.equal(tf.round(self.predicted), self.L)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    " \n",
    "    def train(self, epochs=20, minibatch_size=256):\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            for batch_idx, (features, labels) in enumerate(self._create_minibatches(minibatch_size)):\n",
    "                d = { self.X: features, self.Y: labels }\n",
    "                loss, _, acc = self.sess.run([self.cost, self.optimizer, self.accuracy], feed_dict=d)\n",
    "                \n",
    "                if (batch_idx + 1) % 50 == 0:\n",
    "                    n_treated = batch_idx * len(features)\n",
    "                    p_treated = 100.0 * batch_idx / len(self.train_data[0])\n",
    "                    msg = 'Epoch: {:>2} [{:>5}/{:>5} ({:2.0f}%)]\\tLoss: {:2.6f}\\tAccuracy: {:5.2f}%'\n",
    "                    print(msg.format(epoch, n_treated, len(features), p_treated, loss, acc * 100))\n",
    "            \n",
    "\n",
    "    def predict(self, data):\n",
    "        predictions = []\n",
    "        for features in data:\n",
    "            d = { self.X: features }\n",
    "            pred = self.sess.run(self.predicted, feed_dict=d)\n",
    "            predictions.append(pred)\n",
    "        return predictions\n",
    "\n",
    "    def _create_minibatches(self, minibatch_size):\n",
    "        pos = 0\n",
    "\n",
    "        data, labels = self.train_data\n",
    "        n_samples = len(labels)\n",
    "\n",
    "        batches = []\n",
    "        while pos + minibatch_size < n_samples:\n",
    "            batches.append((data[pos:pos+minibatch_size,:], labels[pos:pos+minibatch_size]))\n",
    "            pos += minibatch_size\n",
    "\n",
    "        if pos < n_samples:\n",
    "            batches.append((data[pos:n_samples,:], labels[pos:n_samples]))\n",
    "\n",
    "        return batches\n",
    "\n",
    "    # def _transform_labels(self, labels):\n",
    "    #    raise NotImplementedError()\n",
    "        \n",
    "\n",
    "    # def _flatten_input(self, data):\n",
    "    #     raise NotImplementedError()\n",
    "\n",
    "    def _open_session(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _draw_graph_tensorboard(self):\n",
    "        writer = tf.summary.FileWriter('./tmp', self.sess.graph)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = CNNClassifier((x_train, y_train))\n",
    "predictions = svm.predict(x_test)\n",
    "print('Testing score f1: {}'.format(f1_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "tdsHdlvuen-F"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_missclasifications(preds):\n",
    "    count = 0\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, pred in enumerate(preds):\n",
    "        if pred != np.argmax(y_test[i]):\n",
    "            msg = '{} ({})'\n",
    "            msg = msg.format('Even' if preds[i] else 'Odd', str(y_numbers[i]))\n",
    "        \n",
    "            plt.subplot(2, 5, count + 1)\n",
    "            plt.title(msg)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(x_test[i].reshape(28, 28), cmap=cm.binary)\n",
    "            count += 1\n",
    "            if count == 10:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4zsDrRHlen-M"
   },
   "source": [
    "Some missclassification examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "BIAt37iXen-O"
   },
   "outputs": [],
   "source": [
    "plot_missclasifications(predictions)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST_CNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
