{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yc1aMHdo61K4"
   },
   "source": [
    "# MNIST Odd-Even classification with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HP2KozF61K8"
   },
   "source": [
    "## Data loading\n",
    "\n",
    "Let's load the dataset using the Keras API implementation included in TensorFlow.\n",
    "\n",
    "As we only want to classify the images between **odd** and **even** numbers, we will map labels to *1* if odd or *0* if even."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_v0CkZcZ61K-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Set Odd (1) or Even (0) labels for each sample\n",
    "y_train = np.array(list(map(lambda x: [1, 0] if x%2 else [0, 1], y_train)), dtype=np.float32)\n",
    "y_test = np.array(list(map(lambda x: [1, 0] if x%2 else [0, 1], y_test)), dtype=np.float32)\n",
    "\n",
    "# Normalize images dividing by max pixel value (255)\n",
    "x_train = (x_train / 255.0).astype(np.float32)\n",
    "x_test = (x_test / 255.0).astype(np.float32)\n",
    "\n",
    "# Reshape to TF API (#img, rows, cols, channels)\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqk5XzLt61LE"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EKJgxT9g61LF"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 120\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "51RNAnFA61LL"
   },
   "outputs": [],
   "source": [
    "num_features = 28*28\n",
    "\n",
    "x = tf.placeholder('float', shape=[None, num_features])\n",
    "y = tf.placeholder('float', shape=[None,1])\n",
    "\n",
    "W = tf.Variable(tf.zeros([num_features,1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y_raw = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oVAWymGh61LS"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cp9CCq0t61LU"
   },
   "source": [
    "###Â SVM Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('svm_model', reuse=tf.AUTO_REUSE) as scope:\n",
    "    x_data = tf.placeholder(shape=(None, num_features), dtype=tf.float32)\n",
    "    y_target = tf.placeholder(shape=(None, 1), dtype=tf.float32)\n",
    "\n",
    "    weights = tf.get_variable('W', shape=(num_features, 1), initializer=tf.random_normal_initializer)\n",
    "    bias = tf.get_variable('b', shape=(1, 1), initializer=tf.random_normal_initializer)\n",
    "\n",
    "    output = tf.subtract(tf.matmul(x_data, weights), bias)\n",
    "    \n",
    "    l2_norm = tf.reduce_sum(tf.square(weights))\n",
    "    alpha = tf.constant([0.01])\n",
    "    classification_term = tf.reduce_mean(tf.maximum(0.0, tf.subtract(1.0, tf.multiply(output, y_target))))\n",
    "    loss = tf.add(classification_term, tf.multiply(alpha, l2_norm))\n",
    "    \n",
    "    prediction = tf.sign(output)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, y_target), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for it in range(1, num_iterations + 1):\n",
    "        x_batch, y_true_batch = session.run(next_element)\n",
    "\n",
    "        feed_dict_train = { x_image: x_batch,\n",
    "                            y_true: y_true_batch }\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "        \n",
    "        if it % 100 == 0:\n",
    "            acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "            msg = 'Iteration: {:>4}, Training Accuracy: {:>6.2%}'\n",
    "            print(msg.format(it, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BEAAmdEq61LU"
   },
   "outputs": [],
   "source": [
    "class SVMClassifier:\n",
    "    def __init__(self, train_data, train_labels):\n",
    "        data = self._flatten_input(train_data)\n",
    "        labels = self._transform_labels(train_labels)\n",
    "\n",
    "        self.train_data = (data, labels)\n",
    "        self._assemble_graph()\n",
    "        \n",
    "        self._open_session()\n",
    "        \n",
    "    def train(self):\n",
    "        for batch in self._create_minibatches(BATCH_SIZE):\n",
    "            print(batch)\n",
    "            \n",
    "    \n",
    "    def predict(self, data):\n",
    "        pass\n",
    "    \n",
    "    def _create_placeholders_and_variables(self, num_features)\n",
    "        self.x = tf.placeholder(dtype=tf.float32, shape=(None, num_features))\n",
    "        self.y = tf.placeholder(dtype=tf.float32, shape=(None, 2))\n",
    "        \n",
    "        self.weights = tf.get_variable('W', shape=(None, num_features), initializer=tf.initializers.he_normal)\n",
    "        self.bias = tf.get_variable('b', shape=(None, 1))\n",
    "        \n",
    "        self.loss = tf.\n",
    "    \n",
    "    def _assemble_graph(self, learning_rate = 0.02):\n",
    "        pass\n",
    "\n",
    "    def _create_minibatches(self, minibatch_size):\n",
    "        pos = 0\n",
    "\n",
    "        data, labels = self.train_data\n",
    "        n_samples = len(labels)\n",
    "\n",
    "        batches = []\n",
    "        while pos + minibatch_size < n_samples:\n",
    "            batches.append((data[pos:pos+minibatch_size,:], labels[pos:pos+minibatch_size]))\n",
    "            pos += minibatch_size\n",
    "\n",
    "        if pos < n_samples:\n",
    "            batches.append((data[pos:n_samples,:], labels[pos:n_samples,:]))\n",
    "\n",
    "        return batches\n",
    "\n",
    "    def _transform_labels(self, labels):\n",
    "        pass\n",
    "        \n",
    "    def _flatten_input(self, data):\n",
    "        pass\n",
    "\n",
    "    def _open_session(self):\n",
    "        self.sess = tf.Session()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
